---
title: åˆ›å»ºDeepSeekæœ¬åœ°éƒ¨ç½²Pythonç‰ˆæœ¬
date: 2025-08-07 18:18:13
tag:
    - AI
categories:
    - å¼€å‘ç¼–ç¨‹
---

> ã€€æœ€è¿‘æƒ³åˆ›å»ºä¸€ä¸ªAIç¿»è¯‘æœºå™¨äººï¼ŒåŠ æˆªå›¾ç¿»è¯‘ï¼Œè™½ç„¶æ•ˆæœä¸å¤ªå¥½ï¼Œä½†æ˜¯æœºå™¨äººç¡®å®æå¥½äº†ï¼Œè®°å½•ä¸‹

# å®‰è£…ä¾èµ–ï¼ˆä¸»è¦éš¾ç‚¹ï¼‰
1. é¦–å…ˆç¡®è®¤`Python`çš„ç‰ˆæœ¬ä¸è¦å¤ªæ–°ï¼Œæˆ‘è¿™é‡Œæ˜¯ `3.11`
2. æ‰“å¼€`PyTorch`å®˜ç½‘ https://pytorch.org/get-started/locally/
3. ä¸‹è½½`PyTorch`å®˜ç½‘æ˜¾ç¤ºçš„Cubaç‰ˆæœ¬ https://developer.nvidia.com/cuda-toolkit-archive
4. å®‰è£…`PyTorch`å®˜ç½‘æ˜¾ç¤ºçš„å‘½ä»¤
## `llama_cpp` åº“ å®‰è£…
```powershell
$env:CMAKE_ARGS="-DGGML_CUDA=on"
pip install llama-cpp-python[server] --upgrade --force-reinstall --no-cache-dir
```
å¦‚æœä½ å‡ºé—®é¢˜äº†ï¼Œè¯·æ£€æŸ¥`CUDA`,`Python`ç‰ˆæœ¬,`PyTorch`ç‰ˆæœ¬æ˜¯å¦åŒ¹é…

## ä¸‹è½½DeepSeekæ¨¡å‹
ä¸€èˆ¬ä» https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-GGUF è·å–
å›½å†…å¯ä»¥ç”¨ï¼š
https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/master/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf
https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/master/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf

## ä»£ç 
```python
import time
from llama_cpp import Llama


# ===== ç¡¬ä»¶ä¼˜åŒ–é…ç½® =====
MODEL_PATH = r"models\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
GPU_LAYERS = 30  # RTX 3050 Ti æ¨è30å±‚
CPU_THREADS = 20   # 3900Xåˆ†é…6çº¿ç¨‹
MAX_TOKENS = 4096 # æœ€å¤§ç”Ÿæˆé•¿åº¦

# ===== R1ä¸“ç”¨å‚æ•° =====
SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘å®˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯å°†ä¸‹é¢æä¾›ç»™ä½ çš„æ–‡æœ¬ç¿»è¯‘æˆå¯ä¾›ä¸­å›½äººé˜…è¯»çš„ä¸­æ–‡ï¼Œåªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–é¢å¤–çš„è¾“å‡º"
# ===== æ¨¡å‹åˆå§‹åŒ– =====
print("â³ æ­£åœ¨åŠ è½½DeepSeek-R1æ¨¡å‹ï¼Œè¯·ç¨å€™...")
start_time = time.time()

llm = Llama(
    model_path=MODEL_PATH,
    n_gpu_layers=30,
    n_threads=20,
    n_ctx=4096,
    n_batch=512,
    verbose=True
)

load_time = time.time() - start_time
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶: {load_time:.1f}ç§’")

# ===== R1ä¸“ç”¨å¯¹è¯æ ¼å¼ =====
def format_r1_prompt(user_input):
    return [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_input}
    ]

# ===== æµå¼è¾“å‡ºå‡½æ•° =====
def generate_response(prompt):
    try:
        response = llm.create_chat_completion(
            messages=format_r1_prompt(prompt),
            max_tokens=MAX_TOKENS,
            temperature=0.7,
            stop=["<|im_end|>"],
            stream=True
        )
        
        full_response = ""
        for chunk in response:
            delta = chunk['choices'][0]['delta']
            if 'content' in delta:
                content = delta['content']
                print(content, end='', flush=True)
                full_response += content
        return full_response
    except Exception as e:
        return f"âŒ ç”Ÿæˆé”™è¯¯: {str(e)}"

# ===== ä¸»äº¤äº’å¾ªç¯ =====
print("\nğŸ¤– DeepSeek-R1åŠ©æ‰‹å·²å°±ç»ªï¼ˆè¾“å…¥'exit'é€€å‡ºï¼‰")
print("ğŸ’¡ æç¤ºï¼šR1æ˜¯é€šç”¨å¯¹è¯æ¨¡å‹ï¼Œé€‚åˆå„ç±»é—®é¢˜\n")

while True:
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nğŸ‘¤ æ‚¨: ")
        
        if user_input.lower() == 'exit':
            print("ğŸ›‘ åŠ©æ‰‹å·²é€€å‡º")
            break
        
        # ç”Ÿæˆå›å¤
        print("ğŸ¤– R1: ", end='', flush=True)
        start_gen = time.time()
        
        # æµå¼è¾“å‡º
        response = generate_response(user_input)
        
        # æ€§èƒ½ç»Ÿè®¡
        gen_time = time.time() - start_gen
        tokens = len(llm.tokenize(response.encode()))
        print(f"\nâ±ï¸ ç”Ÿæˆè€—æ—¶: {gen_time:.1f}s | ğŸ“ ä»¤ç‰Œæ•°: {tokens}")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ“ä½œå·²ä¸­æ–­")
        break

```